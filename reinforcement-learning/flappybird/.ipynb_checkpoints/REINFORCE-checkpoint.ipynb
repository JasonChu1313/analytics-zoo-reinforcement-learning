{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/skimage/viewer/utils/core.py:10: UserWarning: Recommended matplotlib backend is `Agg` for full skimage.viewer functionality.\n",
      "  warn(\"Recommended matplotlib backend is `Agg` for full \"\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import skimage as skimage\n",
    "from skimage import transform, color, exposure\n",
    "from skimage.transform import rotate\n",
    "from skimage.viewer import ImageViewer\n",
    "\n",
    "import sys\n",
    "import os\n",
    "work_space = os.getcwd()\n",
    "\n",
    "#work_space = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "sys.path.append(work_space+\"/game/\")\n",
    "import wrapped_flappy_bird as game\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import json\n",
    "import os\n",
    "\n",
    "from bigdl.optim.optimizer import *\n",
    "from bigdl.nn.criterion import *\n",
    "from zoo.pipeline.api.keras.models import Sequential\n",
    "from zoo.pipeline.api.keras.layers import Dense, Dropout, Activation,Flatten,Convolution2D\n",
    "import tensorflow as tf\n",
    "sc = get_spark_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_RDD(X, y):\n",
    "    return sc.parallelize(X).zip(sc.parallelize(y)).map(\n",
    "            lambda x: Sample.from_ndarray(x[0],x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(advantages, smallEps=1e-8):\n",
    "    return (advantages - advantages.mean())/(advantages.std() + smallEps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTION_PER_FRAME = 1\n",
    "IMAGE_ROWS, IMAGE_COLS = 80, 80\n",
    "IMAGE_CHANNELS = 4\n",
    "ACTION_SIZE = 2\n",
    "LEARNING_RATE = 1e-4\n",
    "INITIAL_EPSILON = 0.1\n",
    "FINAL_EPSILON = 0.0001\n",
    "EXPLORE = 3000000\n",
    "BATCH_SIZE = 1 # every how many eposides do a parameter update\n",
    "GAMMA = 0.99\n",
    "r_reward_moving_average = 0\n",
    "average_step_record = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the abstraction of birdagent\n",
    "class BirdAgent:\n",
    "    def __init__(self, state_size, action_size, gamma=0.95):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = gamma\n",
    "        self.model = self._build_mode()\n",
    "\n",
    "    def _build_mode(self):\n",
    "        print(\"Now we build the model\")\n",
    "        model = Sequential()\n",
    "        model.add(Convolution2D(32, 8, 8, subsample=(4, 4), border_mode='same',\n",
    "                                input_shape=(IMAGE_ROWS, IMAGE_COLS, IMAGE_CHANNELS)))  # 80*80*4\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Convolution2D(64, 4, 4, subsample=(2, 2), border_mode='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Convolution2D(64, 3, 3, subsample=(1, 1), border_mode='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(2))\n",
    "        \n",
    "        # get the 1 * 2 output represent each action's probability\n",
    "        model.add(Activation('softmax'))\n",
    "        return model\n",
    "\n",
    "    # sample the action from the predict of the model\n",
    "    def action_sampler(self, out):\n",
    "        # return 1 if result > np.random.random() else 0\n",
    "        return np.random.choice([0, 1], p=out)\n",
    "\n",
    "    def act(self, state, epsilon = 0.01):\n",
    "        explore = False\n",
    "        random_num = 1\n",
    "        # random_num = random.random()\n",
    "        action = np.zeros(2)\n",
    "        if random_num <= epsilon:\n",
    "            explore = True\n",
    "            # ramdomly select an action\n",
    "            action_index = random.randrange(ACTION_SIZE)\n",
    "            action[action_index] = 1\n",
    "            print(\"*********** Random Action *********** : \", action_index)\n",
    "            return action,action,explore\n",
    "\n",
    "        else:\n",
    "            if isinstance(state, np.ndarray):\n",
    "                features = to_sample_rdd(state, np.zeros([state.shape[0]]))\n",
    "            out = self.model.predict(features)\n",
    "            #print(\"out type\",out)\n",
    "            out = out.collect()\n",
    "            print(\"out result \",out)\n",
    "            \n",
    "            if self.action_sampler(np.squeeze(out)) == 0:\n",
    "                action[0] = 1\n",
    "            else:\n",
    "                action[1] = 1\n",
    "            return np.squeeze(out),action,explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sample_rdd(x, y, numSlices=None):\n",
    "    \"\"\"\n",
    "    Conver x and y into RDD[Sample]\n",
    "    :param x: ndarray and the first dimension should be batch\n",
    "    :param y: ndarray and the first dimension should be batch\n",
    "    :param numSlices:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    from bigdl.util.common import Sample\n",
    "    x_rdd = sc.parallelize(x, numSlices)\n",
    "    y_rdd = sc.parallelize(y, numSlices)\n",
    "    return x_rdd.zip(y_rdd).map(lambda item: Sample.from_ndarray(item[0], item[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the abstraction of rollouts\n",
    "class RollOuts():\n",
    "    def __init__(self, next_states, actions, rewards, logprobs, gradients, total_steps):\n",
    "        self.next_states = next_states\n",
    "        self.actions = actions\n",
    "        self.rewards = rewards\n",
    "        self.steps = total_steps\n",
    "        self.logprobs = logprobs\n",
    "        self.gradients = gradients\n",
    "        self.total_rewards = np.sum(rewards)\n",
    "\n",
    "\n",
    "    def get_summary(self):\n",
    "        return {\" total_reward \": self.total_rewards,\n",
    "                \" total_steps \": self.steps}\n",
    "\n",
    "\n",
    "    def prepare_target(self):\n",
    "        result = []\n",
    "        for action, adv in list(zip(self.actions, self.advs)):\n",
    "            adv = adv * action\n",
    "            print('action and advantages : ',action, adv)\n",
    "            result.append(adv)\n",
    "        return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the abstraction of experience memory\n",
    "class ExperienceStroe:\n",
    "    def __init__(self):\n",
    "        self.rollouts = []\n",
    "\n",
    "    def add_rollout(self, next_states, actions, rewards,logprobs, gradients, total_steps):\n",
    "        self.rollouts.append(RollOuts(next_states, actions, rewards, logprobs, gradients,total_steps))\n",
    "\n",
    "    def num_experiences(self):\n",
    "        return len(self.rollouts)\n",
    "\n",
    "    def get_range(self, start, end):\n",
    "        return self.rollouts[start:end]\n",
    "\n",
    "    def reset(self):\n",
    "        self.rollouts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_summary(rollouts, records, verbose=True):\n",
    "    rollout_rewards = np.array([rollout.total_rewards for rollout in rollouts])\n",
    "    print(\"reward mean %s\" % (rollout_rewards.mean()))\n",
    "    print(\"reward std %s\" % (rollout_rewards.std()))\n",
    "    print(\"reward max %s\" % (rollout_rewards.max()))\n",
    "    records.append([rollout_rewards.mean(), rollout_rewards.std()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate running reward\n",
    "def cal_reward(rollouts, gamma, discounted=True):\n",
    "    for rollout in rollouts:\n",
    "        rewards = rollout.rewards\n",
    "        r_reward = []\n",
    "        running_reward = 0\n",
    "        for reward in rewards[::-1]:\n",
    "            # discounted reward\n",
    "            # reward = (-1) * reward\n",
    "            if discounted == True:\n",
    "                running_reward = gamma * running_reward + reward\n",
    "            else:\n",
    "                running_reward = running_reward + reward\n",
    "            r_reward.append(running_reward)\n",
    "        rollout.r_rewards = np.squeeze(np.array(np.vstack(r_reward[::-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_advantage_moving_average(rollouts):\n",
    "    global r_reward_moving_average\n",
    "    for rollout in rollouts:\n",
    "        advs = np.zeros([rollout.rewards.shape[0]])\n",
    "        i = 0\n",
    "        for r_reward in rollout.r_rewards:\n",
    "            print(\"running_rewards and r_reward_moving_average\",r_reward ,r_reward_moving_average)\n",
    "            # calculate moving average\n",
    "            r_reward_moving_average = 0.9 * r_reward_moving_average + (1-0.9) * r_reward\n",
    "            # correct the bias\n",
    "            advs[i] = r_reward - r_reward_moving_average\n",
    "            i += 1\n",
    "            # rollout.advs = normalize(advs)   \n",
    "            rollout.advs=advs\n",
    "        print(\"advantage \",advs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the advantage = reward - expected reward at this time step\n",
    "def cal_advantage(rollouts):\n",
    "    max_steps = max(rollout.rewards.shape[0] for rollout in rollouts)\n",
    "    for rollout in rollouts:\n",
    "        rollout.r_rewards = np.pad(rollout.r_rewards, (0, max_steps - rollout.r_rewards.shape[0]),'constant')\n",
    "    baselines = np.mean(np.vstack([rollout.r_rewards for rollout in rollouts]), axis=0)\n",
    "    for rollout in rollouts:\n",
    "        rollout.advs = rollout.r_rewards - baselines\n",
    "        rollout.advs = rollout.advs[:len(rollout.r_rewards)]\n",
    "        rollout.r_rewards = rollout.r_rewards[:len(rollout.rewards)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(agent, render=False):\n",
    "    # 1. initialize\n",
    "    # open up a game state to communicate with emulator\n",
    "    game_state = game.GameState()\n",
    "    # used to save the actions, should be a 1*2 nparray and sum(action) should be 1\n",
    "    actions = np.zeros([1,2])\n",
    "    # used to save the rewards\n",
    "    rewards = np.array([])\n",
    "    # get the first state by doing nothing and preprocess the image to 80x80x4\n",
    "  \n",
    "    gradients = np.zeros([1, 2])\n",
    "    # should be (1*2) nparray and each col represent the probability to chose that action\n",
    "    logprobs = np.array([1, 2])\n",
    "\n",
    "    do_nothing = np.zeros(ACTION_SIZE)\n",
    "    do_nothing[0] = 1\n",
    "    x_t, r_0, terminal = game_state.frame_step(do_nothing)\n",
    "\n",
    "    # preprocess the first frame\n",
    "    x_t = skimage.color.rgb2gray(x_t)\n",
    "    x_t = skimage.transform.resize(x_t, (80, 80))\n",
    "    x_t = skimage.exposure.rescale_intensity(x_t, out_range=(0, 255))\n",
    "\n",
    "    # rescale to 0-1\n",
    "    x_t = x_t / 255.0\n",
    "\n",
    "    # 80 * 80 * 4\n",
    "    state = np.stack((x_t, x_t, x_t, x_t), axis=2)\n",
    "    print (state.shape)\n",
    "    # In Keras, need to reshape 1 * 80 * 80 * 4\n",
    "    state = state.reshape(1, state.shape[0], state.shape[1], state.shape[2])  # 1*80*80*4\n",
    "    # the observations will be batch_size * 80 * 80 * 4\n",
    "    observations = np.zeros([1, state.shape[1], state.shape[2], state.shape[3]])\n",
    "    for step in range(1, 500):\n",
    "        # next state's shape, should be (1*80*80*4)\n",
    "        \n",
    "        # get the state list\n",
    "        observations = np.vstack((observations, state))\n",
    "        # predict the action\n",
    "\n",
    "        logprob,action,explore = agent.act(state)\n",
    "        \n",
    "        \n",
    "        actions = np.vstack((actions, action))\n",
    "        logprobs = np.vstack((logprobs ,logprob))\n",
    "        gradients = np.vstack((gradients , action.astype('float32') - logprob))\n",
    "        # use the predicted action to determine the next state\n",
    "        x_t1_colored, reward, terminal = game_state.frame_step(action)\n",
    "        # print ('reward ',reward)\n",
    "        # rgb to gray and rescale\n",
    "        x_t1 = skimage.color.rgb2gray(x_t1_colored)\n",
    "        x_t1 = skimage.transform.resize(x_t1, (80, 80))\n",
    "        x_t1 = skimage.exposure.rescale_intensity(x_t1, out_range=(0, 255))\n",
    "        # rescale to 0-1\n",
    "        x_t1 = x_t1 / 255\n",
    "        # update state\n",
    "        x_t1 = x_t1.reshape(1,x_t1.shape[0],x_t1.shape[1],1)\n",
    "        state = np.append(x_t1,state[:,:,:,:3],axis=3)\n",
    "        rewards = np.append(rewards, reward)\n",
    "\n",
    "        print('living steps : ',observations.shape[0])\n",
    "        # print('state size ',state.shape)\n",
    "        # print('action size : ',action.shape)\n",
    "        # print('action : ',action)\n",
    "        # print('reward : ',reward)\n",
    "        # print('terminal : ',terminal)\n",
    "        if terminal or step == 498:\n",
    "            break\n",
    "    return observations[1:], actions[1:], rewards , logprobs[1:], gradients[1:], step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_n_games(agent, history, n=4, verbose=True):\n",
    "    start_eps = history.num_experiences()\n",
    "    total_step = 0\n",
    "    for i in range(n):\n",
    "        observations, actions, rewards, logprobs, gradients, step = play_game(agent=agent)\n",
    "        history.add_rollout(observations, actions, rewards, logprobs, gradients, step)\n",
    "        total_step += step\n",
    "    end_eps = history.num_experiences()\n",
    "    return start_eps, end_eps, total_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(agent, rollouts):\n",
    "    cal_reward(rollouts, agent.gamma)\n",
    "    cal_advantage_moving_average(rollouts)\n",
    "    \n",
    "    #copy_r_reward_to_advs(rollouts)\n",
    "    # cal_advantage(rollouts)\n",
    "    X_batch = np.zeros([1, 80, 80, 4])\n",
    "    Y_batch = np.array([0,0])\n",
    "    for rollout in rollouts:\n",
    "        X_batch = np.vstack((X_batch, rollout.next_states))\n",
    "        # Y_batch: the label of the training should be [[action,adv]]\n",
    "        Y_batch = np.vstack((Y_batch, rollout.prepare_target()))\n",
    "    X_batch = X_batch[1:]\n",
    "    Y_batch = Y_batch[1:]\n",
    "    # Y_batch[:,1] = normalize(Y_batch[:,1])\n",
    "\n",
    "    # Y_batch[:, 1] = normalize(y_batch[:, 1])\n",
    "    # prepare to train the model\n",
    "    print('X_batch size : ',X_batch.shape)\n",
    "    print('Y_batch size : ',Y_batch.shape)\n",
    "    rdd_sample = to_RDD(X_batch, Y_batch)\n",
    "    batch_size = X_batch.shape[0] - X_batch.shape[0]%8\n",
    "    print (\"using batch_size = \",batch_size)\n",
    "    \n",
    "    \n",
    "    rmsprop = RMSprop(learningrate=1e-5, decayrate=0.9)\n",
    "    agent.model.compile(rmsprop,PGCriterion())\n",
    "    agent.model.fit(rdd_sample,batch_size=8, nb_epoch=1)\n",
    "    \n",
    "    \n",
    "#     optimizer = Optimizer(model=agent.model,\n",
    "#                                   training_rdd=rdd_sample,\n",
    "#                                   criterion=PGCriterion(),\n",
    "#                                   optim_method= RMSprop(learningrate=0.005),\n",
    "#                                   end_trigger=MaxIteration(1),\n",
    "#                                   batch_size=batch_size)\n",
    "#     #else:\n",
    "#         #optimizer.set_traindata(training_rdd=rdd_sample, batch_size=batch_size)\n",
    "#         #optimizer.set_criterion(RFPGCriterion())\n",
    "#     agent.model = optimizer.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we build the model\n",
      "creating: createZooKerasSequential\n",
      "creating: createZooKerasConvolution2D\n",
      "creating: createZooKerasActivation\n",
      "creating: createZooKerasConvolution2D\n",
      "creating: createZooKerasActivation\n",
      "creating: createZooKerasConvolution2D\n",
      "creating: createZooKerasActivation\n",
      "creating: createZooKerasFlatten\n",
      "creating: createZooKerasDense\n",
      "creating: createZooKerasActivation\n",
      "creating: createZooKerasDense\n",
      "creating: createZooKerasActivation\n",
      "(80, 80, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out result  [array([0.49992248, 0.5000775 ], dtype=float32)]\n",
      "living steps :  2\n",
      "out result  [array([0.50175947, 0.4982406 ], dtype=float32)]\n",
      "living steps :  3\n",
      "out result  [array([0.50122267, 0.4987774 ], dtype=float32)]\n",
      "living steps :  4\n",
      "out result  [array([0.4974052 , 0.50259477], dtype=float32)]\n",
      "living steps :  5\n",
      "out result  [array([0.4961809 , 0.50381917], dtype=float32)]\n",
      "living steps :  6\n",
      "out result  [array([0.49512464, 0.5048754 ], dtype=float32)]\n",
      "living steps :  7\n",
      "out result  [array([0.49476895, 0.505231  ], dtype=float32)]\n",
      "living steps :  8\n",
      "out result  [array([0.49414617, 0.50585383], dtype=float32)]\n",
      "living steps :  9\n",
      "out result  [array([0.49528244, 0.5047176 ], dtype=float32)]\n",
      "living steps :  10\n",
      "out result  [array([0.49476132, 0.50523865], dtype=float32)]\n",
      "living steps :  11\n",
      "out result  [array([0.4962023, 0.5037977], dtype=float32)]\n",
      "living steps :  12\n",
      "out result  [array([0.4993974 , 0.50060254], dtype=float32)]\n",
      "living steps :  13\n",
      "out result  [array([0.5027151, 0.4972849], dtype=float32)]\n",
      "living steps :  14\n",
      "out result  [array([0.4993868, 0.5006132], dtype=float32)]\n",
      "living steps :  15\n",
      "out result  [array([0.50035584, 0.4996441 ], dtype=float32)]\n",
      "living steps :  16\n",
      "out result  [array([0.49886835, 0.50113165], dtype=float32)]\n",
      "living steps :  17\n",
      "out result  [array([0.4978327 , 0.50216734], dtype=float32)]\n",
      "living steps :  18\n",
      "out result  [array([0.50005335, 0.4999467 ], dtype=float32)]\n",
      "living steps :  19\n",
      "out result  [array([0.5001478 , 0.49985218], dtype=float32)]\n",
      "living steps :  20\n",
      "out result  [array([0.4952074, 0.5047926], dtype=float32)]\n",
      "living steps :  21\n",
      "out result  [array([0.49321866, 0.50678134], dtype=float32)]\n",
      "living steps :  22\n",
      "out result  [array([0.49520552, 0.50479454], dtype=float32)]\n",
      "living steps :  23\n",
      "out result  [array([0.49671707, 0.5032829 ], dtype=float32)]\n",
      "living steps :  24\n",
      "out result  [array([0.49926144, 0.50073856], dtype=float32)]\n",
      "living steps :  25\n",
      "out result  [array([0.50103366, 0.4989663 ], dtype=float32)]\n",
      "living steps :  26\n",
      "out result  [array([0.5018506 , 0.49814937], dtype=float32)]\n",
      "living steps :  27\n",
      "out result  [array([0.49713543, 0.5028646 ], dtype=float32)]\n",
      "living steps :  28\n",
      "out result  [array([0.4970956 , 0.50290436], dtype=float32)]\n",
      "living steps :  29\n",
      "out result  [array([0.49661484, 0.5033851 ], dtype=float32)]\n",
      "living steps :  30\n",
      "out result  [array([0.49557537, 0.50442463], dtype=float32)]\n",
      "living steps :  31\n",
      "out result  [array([0.49313298, 0.506867  ], dtype=float32)]\n",
      "living steps :  32\n",
      "out result  [array([0.49338624, 0.5066138 ], dtype=float32)]\n",
      "living steps :  33\n",
      "out result  [array([0.49354297, 0.506457  ], dtype=float32)]\n",
      "living steps :  34\n",
      "out result  [array([0.49314618, 0.50685376], dtype=float32)]\n",
      "living steps :  35\n",
      "out result  [array([0.49351192, 0.506488  ], dtype=float32)]\n",
      "living steps :  36\n",
      "out result  [array([0.4927992, 0.5072008], dtype=float32)]\n",
      "living steps :  37\n",
      "out result  [array([0.49625376, 0.50374615], dtype=float32)]\n",
      "living steps :  38\n",
      "out result  [array([0.49820662, 0.5017934 ], dtype=float32)]\n",
      "living steps :  39\n",
      "out result  [array([0.5006323 , 0.49936762], dtype=float32)]\n",
      "living steps :  40\n",
      "out result  [array([0.5000448 , 0.49995524], dtype=float32)]\n",
      "living steps :  41\n",
      "out result  [array([0.49850178, 0.5014983 ], dtype=float32)]\n",
      "living steps :  42\n",
      "out result  [array([0.49953324, 0.5004667 ], dtype=float32)]\n",
      "living steps :  43\n",
      "out result  [array([0.5023816 , 0.49761832], dtype=float32)]\n",
      "living steps :  44\n",
      "out result  [array([0.50281566, 0.49718437], dtype=float32)]\n",
      "living steps :  45\n",
      "out result  [array([0.50380576, 0.49619427], dtype=float32)]\n",
      "living steps :  46\n",
      "out result  [array([0.5028992 , 0.49710077], dtype=float32)]\n",
      "living steps :  47\n",
      "out result  [array([0.5045422, 0.4954578], dtype=float32)]\n",
      "living steps :  48\n",
      "out result  [array([0.5053393 , 0.49466068], dtype=float32)]\n",
      "living steps :  49\n",
      "out result  [array([0.50586766, 0.4941324 ], dtype=float32)]\n",
      "living steps :  50\n",
      "reward mean 3.8\n",
      "reward std 0.0\n",
      "reward max 3.8\n",
      "used in training:  0 - 1\n",
      "num of total steps in training  49\n",
      "training begin\n",
      "running_rewards and r_reward_moving_average 1.7442272289970748 0\n",
      "running_rewards and r_reward_moving_average 1.7307655042074472 0.17442272289970745\n",
      "running_rewards and r_reward_moving_average 1.7165952675867866 0.3300570010304814\n",
      "running_rewards and r_reward_moving_average 1.7016792290387228 0.4687108276861119\n",
      "running_rewards and r_reward_moving_average 1.6859781358302346 0.5920076678213729\n",
      "running_rewards and r_reward_moving_average 1.6694506692949838 0.7014047146222591\n",
      "running_rewards and r_reward_moving_average 1.652053336099983 0.7982093100895316\n",
      "running_rewards and r_reward_moving_average 1.6337403537894557 0.8835937126905766\n",
      "running_rewards and r_reward_moving_average 1.6144635303046901 0.9586083768004645\n",
      "running_rewards and r_reward_moving_average 1.5941721371628317 1.024193892150887\n",
      "running_rewards and r_reward_moving_average 1.5728127759608754 1.0811917166520817\n",
      "running_rewards and r_reward_moving_average 1.5503292378535531 1.130353822582961\n",
      "running_rewards and r_reward_moving_average 1.5266623556353192 1.1723513641100203\n",
      "running_rewards and r_reward_moving_average 1.501749848037178 1.20778246326255\n",
      "running_rewards and r_reward_moving_average 1.4755261558286084 1.2371792017400127\n",
      "running_rewards and r_reward_moving_average 1.4479222692932718 1.2610138971488722\n",
      "running_rewards and r_reward_moving_average 1.4188655466244966 1.2797047343633121\n",
      "running_rewards and r_reward_moving_average 1.388279522762628 1.2936208155894307\n",
      "running_rewards and r_reward_moving_average 1.3560837081711872 1.3030866863067503\n",
      "running_rewards and r_reward_moving_average 1.3221933770223022 1.308386388493194\n",
      "running_rewards and r_reward_moving_average 1.2865193442340024 1.309767087346105\n",
      "running_rewards and r_reward_moving_average 1.2489677307726341 1.3074423130348947\n",
      "running_rewards and r_reward_moving_average 1.2094397166027726 1.3015948548086687\n",
      "running_rewards and r_reward_moving_average 1.1678312806344975 1.292379340988079\n",
      "running_rewards and r_reward_moving_average 1.1240329269836815 1.2799245349527208\n",
      "running_rewards and r_reward_moving_average 1.077929396824928 1.264335374155817\n",
      "running_rewards and r_reward_moving_average 1.0293993650788715 1.2456947764227282\n",
      "running_rewards and r_reward_moving_average 0.9783151211356543 1.2240652352883425\n",
      "running_rewards and r_reward_moving_average 0.9245422327743731 1.1994902238730736\n",
      "running_rewards and r_reward_moving_average 0.867939192394077 1.1719954247632034\n",
      "running_rewards and r_reward_moving_average 0.8083570446253443 1.1415898015262906\n",
      "running_rewards and r_reward_moving_average 0.7456389943424677 1.108266525836196\n",
      "running_rewards and r_reward_moving_average 0.679619994044703 1.072003772686823\n",
      "running_rewards and r_reward_moving_average 0.61012630952074 1.032765394822611\n",
      "running_rewards and r_reward_moving_average 0.5369750626534106 0.9905014862924238\n",
      "running_rewards and r_reward_moving_average 0.4599737501614848 0.9451488439285226\n",
      "running_rewards and r_reward_moving_average 0.37891973701208925 0.8966313345518188\n",
      "running_rewards and r_reward_moving_average 0.2935997231706202 0.8448601747978458\n",
      "running_rewards and r_reward_moving_average 0.20378918228486337 0.7897341296351232\n",
      "running_rewards and r_reward_moving_average 0.10925177082617196 0.7311396349000973\n",
      "running_rewards and r_reward_moving_average 0.00973870613281258 0.6689508484927047\n",
      "running_rewards and r_reward_moving_average -0.09501188828124993 0.6030296342567155\n",
      "running_rewards and r_reward_moving_average -0.20527567187499993 0.533225482002919\n",
      "running_rewards and r_reward_moving_average -0.32134281249999996 0.4593753666151271\n",
      "running_rewards and r_reward_moving_average -0.44351874999999996 0.38130354870361444\n",
      "running_rewards and r_reward_moving_average -0.572125 0.298821318833253\n",
      "running_rewards and r_reward_moving_average -0.7075 0.21172668694992774\n",
      "running_rewards and r_reward_moving_average -0.85 0.119804018254935\n",
      "running_rewards and r_reward_moving_average -1.0 0.02282361642944153\n",
      "advantage  [ 1.56980451  1.4007085   1.24788444  1.10967156  0.98457342  0.87124136\n",
      "  0.76845962  0.67513198  0.59026964  0.51298042  0.44245895  0.37797787\n",
      "  0.31887989  0.26457065  0.21451226  0.16821753  0.12524473  0.08519284\n",
      "  0.04769732  0.01242629 -0.02092297 -0.05262712 -0.08293962 -0.11209325\n",
      " -0.14030245 -0.16776538 -0.19466587 -0.2211751  -0.24745319 -0.27365061\n",
      " -0.29990948 -0.32636478 -0.3531454  -0.38037518 -0.40817378 -0.43665758\n",
      " -0.46594044 -0.49613441 -0.52735045 -0.55969908 -0.59329093 -0.62823737\n",
      " -0.66465104 -0.70264636 -0.74234007 -0.78385169 -0.82730402 -0.87282362\n",
      " -0.92054125]\n",
      "action and advantages :  [0. 1.] [0.         1.56980451]\n",
      "action and advantages :  [0. 1.] [0.        1.4007085]\n",
      "action and advantages :  [1. 0.] [1.24788444 0.        ]\n",
      "action and advantages :  [1. 0.] [1.10967156 0.        ]\n",
      "action and advantages :  [1. 0.] [0.98457342 0.        ]\n",
      "action and advantages :  [1. 0.] [0.87124136 0.        ]\n",
      "action and advantages :  [1. 0.] [0.76845962 0.        ]\n",
      "action and advantages :  [0. 1.] [0.         0.67513198]\n",
      "action and advantages :  [0. 1.] [0.         0.59026964]\n",
      "action and advantages :  [0. 1.] [0.         0.51298042]\n",
      "action and advantages :  [0. 1.] [0.         0.44245895]\n",
      "action and advantages :  [0. 1.] [0.         0.37797787]\n",
      "action and advantages :  [0. 1.] [0.         0.31887989]\n",
      "action and advantages :  [1. 0.] [0.26457065 0.        ]\n",
      "action and advantages :  [1. 0.] [0.21451226 0.        ]\n",
      "action and advantages :  [1. 0.] [0.16821753 0.        ]\n",
      "action and advantages :  [0. 1.] [0.         0.12524473]\n",
      "action and advantages :  [0. 1.] [0.         0.08519284]\n",
      "action and advantages :  [1. 0.] [0.04769732 0.        ]\n",
      "action and advantages :  [1. 0.] [0.01242629 0.        ]\n",
      "action and advantages :  [0. 1.] [-0.         -0.02092297]\n",
      "action and advantages :  [1. 0.] [-0.05262712 -0.        ]\n",
      "action and advantages :  [1. 0.] [-0.08293962 -0.        ]\n",
      "action and advantages :  [0. 1.] [-0.         -0.11209325]\n",
      "action and advantages :  [0. 1.] [-0.         -0.14030245]\n",
      "action and advantages :  [0. 1.] [-0.         -0.16776538]\n",
      "action and advantages :  [1. 0.] [-0.19466587 -0.        ]\n",
      "action and advantages :  [0. 1.] [-0.        -0.2211751]\n",
      "action and advantages :  [1. 0.] [-0.24745319 -0.        ]\n",
      "action and advantages :  [1. 0.] [-0.27365061 -0.        ]\n",
      "action and advantages :  [0. 1.] [-0.         -0.29990948]\n",
      "action and advantages :  [0. 1.] [-0.         -0.32636478]\n",
      "action and advantages :  [0. 1.] [-0.        -0.3531454]\n",
      "action and advantages :  [1. 0.] [-0.38037518 -0.        ]\n",
      "action and advantages :  [0. 1.] [-0.         -0.40817378]\n",
      "action and advantages :  [0. 1.] [-0.         -0.43665758]\n",
      "action and advantages :  [1. 0.] [-0.46594044 -0.        ]\n",
      "action and advantages :  [1. 0.] [-0.49613441 -0.        ]\n",
      "action and advantages :  [1. 0.] [-0.52735045 -0.        ]\n",
      "action and advantages :  [1. 0.] [-0.55969908 -0.        ]\n",
      "action and advantages :  [0. 1.] [-0.         -0.59329093]\n",
      "action and advantages :  [1. 0.] [-0.62823737 -0.        ]\n",
      "action and advantages :  [1. 0.] [-0.66465104 -0.        ]\n",
      "action and advantages :  [0. 1.] [-0.         -0.70264636]\n",
      "action and advantages :  [0. 1.] [-0.         -0.74234007]\n",
      "action and advantages :  [1. 0.] [-0.78385169 -0.        ]\n",
      "action and advantages :  [1. 0.] [-0.82730402 -0.        ]\n",
      "action and advantages :  [0. 1.] [-0.         -0.87282362]\n",
      "action and advantages :  [1. 0.] [-0.92054125 -0.        ]\n",
      "X_batch size :  (49, 80, 80, 4)\n",
      "Y_batch size :  (49, 2)\n",
      "using batch_size =  48\n",
      "creating: createRMSprop\n",
      "creating: createPGCriterion\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 80, 4)\n",
      "out result  [array([0.4934194, 0.5065806], dtype=float32)]\n",
      "living steps :  2\n",
      "out result  [array([0.49446034, 0.5055396 ], dtype=float32)]\n",
      "living steps :  3\n",
      "out result  [array([0.49350408, 0.50649595], dtype=float32)]\n",
      "living steps :  4\n",
      "out result  [array([0.49254942, 0.5074506 ], dtype=float32)]\n",
      "living steps :  5\n",
      "out result  [array([0.49065623, 0.50934374], dtype=float32)]\n",
      "living steps :  6\n",
      "out result  [array([0.49066597, 0.5093341 ], dtype=float32)]\n",
      "living steps :  7\n",
      "out result  [array([0.4899517, 0.5100483], dtype=float32)]\n",
      "living steps :  8\n",
      "out result  [array([0.48931023, 0.51068974], dtype=float32)]\n",
      "living steps :  9\n",
      "out result  [array([0.4901127 , 0.50988734], dtype=float32)]\n",
      "living steps :  10\n",
      "out result  [array([0.48847577, 0.51152426], dtype=float32)]\n",
      "living steps :  11\n",
      "out result  [array([0.48816925, 0.51183075], dtype=float32)]\n",
      "living steps :  12\n",
      "out result  [array([0.49056157, 0.5094384 ], dtype=float32)]\n",
      "living steps :  13\n",
      "out result  [array([0.49272034, 0.50727963], dtype=float32)]\n",
      "living steps :  14\n",
      "out result  [array([0.49189788, 0.5081021 ], dtype=float32)]\n",
      "living steps :  15\n",
      "out result  [array([0.49362758, 0.5063724 ], dtype=float32)]\n",
      "living steps :  16\n",
      "out result  [array([0.49239862, 0.50760144], dtype=float32)]\n",
      "living steps :  17\n",
      "out result  [array([0.4920932 , 0.50790685], dtype=float32)]\n",
      "living steps :  18\n",
      "out result  [array([0.4928831, 0.507117 ], dtype=float32)]\n",
      "living steps :  19\n",
      "out result  [array([0.49504474, 0.5049553 ], dtype=float32)]\n",
      "living steps :  20\n",
      "out result  [array([0.4917639, 0.5082361], dtype=float32)]\n",
      "living steps :  21\n",
      "out result  [array([0.48839074, 0.5116093 ], dtype=float32)]\n",
      "living steps :  22\n",
      "out result  [array([0.48817337, 0.51182663], dtype=float32)]\n",
      "living steps :  23\n",
      "out result  [array([0.48807043, 0.5119295 ], dtype=float32)]\n",
      "living steps :  24\n",
      "out result  [array([0.49008715, 0.50991285], dtype=float32)]\n",
      "living steps :  25\n",
      "out result  [array([0.48963034, 0.5103697 ], dtype=float32)]\n",
      "living steps :  26\n",
      "out result  [array([0.49050403, 0.509496  ], dtype=float32)]\n",
      "living steps :  27\n",
      "out result  [array([0.48824814, 0.51175183], dtype=float32)]\n",
      "living steps :  28\n",
      "out result  [array([0.48994341, 0.5100566 ], dtype=float32)]\n",
      "living steps :  29\n",
      "out result  [array([0.48961413, 0.5103859 ], dtype=float32)]\n",
      "living steps :  30\n",
      "out result  [array([0.48641878, 0.5135812 ], dtype=float32)]\n",
      "living steps :  31\n",
      "out result  [array([0.48707104, 0.51292896], dtype=float32)]\n",
      "living steps :  32\n",
      "out result  [array([0.4886188 , 0.51138127], dtype=float32)]\n",
      "living steps :  33\n",
      "out result  [array([0.4892993 , 0.51070076], dtype=float32)]\n",
      "living steps :  34\n",
      "out result  [array([0.48723, 0.51277], dtype=float32)]\n",
      "living steps :  35\n",
      "out result  [array([0.48768806, 0.512312  ], dtype=float32)]\n",
      "living steps :  36\n",
      "out result  [array([0.4879818, 0.5120182], dtype=float32)]\n",
      "living steps :  37\n",
      "out result  [array([0.49062037, 0.5093796 ], dtype=float32)]\n",
      "living steps :  38\n",
      "out result  [array([0.4905015 , 0.50949854], dtype=float32)]\n",
      "living steps :  39\n",
      "out result  [array([0.4927462 , 0.50725377], dtype=float32)]\n",
      "living steps :  40\n",
      "out result  [array([0.4918755, 0.5081245], dtype=float32)]\n",
      "living steps :  41\n",
      "out result  [array([0.49379095, 0.5062091 ], dtype=float32)]\n",
      "living steps :  42\n",
      "out result  [array([0.4942272 , 0.50577277], dtype=float32)]\n",
      "living steps :  43\n",
      "out result  [array([0.49673074, 0.5032692 ], dtype=float32)]\n",
      "living steps :  44\n",
      "out result  [array([0.49584353, 0.5041565 ], dtype=float32)]\n",
      "living steps :  45\n",
      "out result  [array([0.4992977, 0.5007023], dtype=float32)]\n",
      "living steps :  46\n",
      "out result  [array([0.49636266, 0.5036373 ], dtype=float32)]\n",
      "living steps :  47\n",
      "out result  [array([0.49961674, 0.5003832 ], dtype=float32)]\n",
      "living steps :  48\n",
      "out result  [array([0.50156736, 0.49843258], dtype=float32)]\n",
      "living steps :  49\n",
      "out result  [array([0.50224966, 0.49775037], dtype=float32)]\n",
      "living steps :  50\n",
      "reward mean 3.8\n",
      "reward std 0.0\n",
      "reward max 3.8\n",
      "used in training:  1 - 2\n",
      "num of total steps in training  49\n",
      "training begin\n",
      "running_rewards and r_reward_moving_average 1.7442272289970748 -0.0794587452135026\n",
      "running_rewards and r_reward_moving_average 1.7307655042074472 0.10290985220755511\n",
      "running_rewards and r_reward_moving_average 1.7165952675867866 0.26569541740754427\n",
      "running_rewards and r_reward_moving_average 1.7016792290387228 0.41078540242546846\n",
      "running_rewards and r_reward_moving_average 1.6859781358302346 0.5398747850867939\n",
      "running_rewards and r_reward_moving_average 1.6694506692949838 0.6544851201611379\n",
      "running_rewards and r_reward_moving_average 1.652053336099983 0.7559816750745225\n",
      "running_rewards and r_reward_moving_average 1.6337403537894557 0.8455888411770686\n",
      "running_rewards and r_reward_moving_average 1.6144635303046901 0.9244039924383073\n",
      "running_rewards and r_reward_moving_average 1.5941721371628317 0.9934099462249455\n",
      "running_rewards and r_reward_moving_average 1.5728127759608754 1.0534861653187342\n",
      "running_rewards and r_reward_moving_average 1.5503292378535531 1.1054188263829483\n",
      "running_rewards and r_reward_moving_average 1.5266623556353192 1.1499098675300088\n",
      "running_rewards and r_reward_moving_average 1.501749848037178 1.1875851163405398\n",
      "running_rewards and r_reward_moving_average 1.4755261558286084 1.2190015895102035\n",
      "running_rewards and r_reward_moving_average 1.4479222692932718 1.2446540461420441\n",
      "running_rewards and r_reward_moving_average 1.4188655466244966 1.2649808684571668\n",
      "running_rewards and r_reward_moving_average 1.388279522762628 1.2803693362739\n",
      "running_rewards and r_reward_moving_average 1.3560837081711872 1.2911603549227728\n",
      "running_rewards and r_reward_moving_average 1.3221933770223022 1.2976526902476142\n",
      "running_rewards and r_reward_moving_average 1.2865193442340024 1.300106758925083\n",
      "running_rewards and r_reward_moving_average 1.2489677307726341 1.298748017455975\n",
      "running_rewards and r_reward_moving_average 1.2094397166027726 1.2937699887876408\n",
      "running_rewards and r_reward_moving_average 1.1678312806344975 1.285336961569154\n",
      "running_rewards and r_reward_moving_average 1.1240329269836815 1.2735863934756881\n",
      "running_rewards and r_reward_moving_average 1.077929396824928 1.2586310468264874\n",
      "running_rewards and r_reward_moving_average 1.0293993650788715 1.2405608818263316\n",
      "running_rewards and r_reward_moving_average 0.9783151211356543 1.2194447301515856\n",
      "running_rewards and r_reward_moving_average 0.9245422327743731 1.1953317692499925\n",
      "running_rewards and r_reward_moving_average 0.867939192394077 1.1682528156024305\n",
      "running_rewards and r_reward_moving_average 0.8083570446253443 1.138221453281595\n",
      "running_rewards and r_reward_moving_average 0.7456389943424677 1.10523501241597\n",
      "running_rewards and r_reward_moving_average 0.679619994044703 1.0692754106086197\n",
      "running_rewards and r_reward_moving_average 0.61012630952074 1.030309868952228\n",
      "running_rewards and r_reward_moving_average 0.5369750626534106 0.9882915130090792\n",
      "running_rewards and r_reward_moving_average 0.4599737501614848 0.9431598679735125\n",
      "running_rewards and r_reward_moving_average 0.37891973701208925 0.8948412561923097\n",
      "running_rewards and r_reward_moving_average 0.2935997231706202 0.8432491042742877\n",
      "running_rewards and r_reward_moving_average 0.20378918228486337 0.7882841661639209\n",
      "running_rewards and r_reward_moving_average 0.10925177082617196 0.7298346677760152\n",
      "running_rewards and r_reward_moving_average 0.00973870613281258 0.667776378081031\n",
      "running_rewards and r_reward_moving_average -0.09501188828124993 0.6019726108862091\n",
      "running_rewards and r_reward_moving_average -0.20527567187499993 0.5322741609694632\n",
      "running_rewards and r_reward_moving_average -0.32134281249999996 0.45851917768501693\n",
      "running_rewards and r_reward_moving_average -0.44351874999999996 0.38053297866651525\n",
      "running_rewards and r_reward_moving_average -0.572125 0.29812780579986375\n",
      "running_rewards and r_reward_moving_average -0.7075 0.21110252521987738\n",
      "running_rewards and r_reward_moving_average -0.85 0.11924227269788967\n",
      "running_rewards and r_reward_moving_average -1.0 0.02231804542810073\n",
      "advantage  [ 1.64131738  1.46507009  1.30580987  1.16180444  1.03149302  0.91346899\n",
      "  0.80646449  0.70933636  0.62105358  0.54068597  0.46739395  0.40041937\n",
      "  0.33907724  0.28274826  0.23087211  0.1829414   0.13849621  0.09711917\n",
      "  0.05843102  0.02208662 -0.01222867 -0.04480226 -0.07589724 -0.10575511\n",
      " -0.13459812 -0.16263149 -0.19004537 -0.21701665 -0.24371058 -0.27028226\n",
      " -0.29687797 -0.32363642 -0.35068987 -0.3781652  -0.40618481 -0.43486751\n",
      " -0.46432937 -0.49468444 -0.52604549 -0.55852461 -0.5922339  -0.62728605\n",
      " -0.66379485 -0.70187579 -0.74164656 -0.78322753 -0.82674227 -0.87231805\n",
      " -0.92008624]\n",
      "action and advantages :  [1. 0.] [1.64131738 0.        ]\n",
      "action and advantages :  [1. 0.] [1.46507009 0.        ]\n",
      "action and advantages :  [1. 0.] [1.30580987 0.        ]\n",
      "action and advantages :  [0. 1.] [0.         1.16180444]\n",
      "action and advantages :  [0. 1.] [0.         1.03149302]\n",
      "action and advantages :  [1. 0.] [0.91346899 0.        ]\n",
      "action and advantages :  [0. 1.] [0.         0.80646449]\n",
      "action and advantages :  [1. 0.] [0.70933636 0.        ]\n",
      "action and advantages :  [1. 0.] [0.62105358 0.        ]\n",
      "action and advantages :  [0. 1.] [0.         0.54068597]\n",
      "action and advantages :  [1. 0.] [0.46739395 0.        ]\n",
      "action and advantages :  [1. 0.] [0.40041937 0.        ]\n",
      "action and advantages :  [0. 1.] [0.         0.33907724]\n",
      "action and advantages :  [1. 0.] [0.28274826 0.        ]\n",
      "action and advantages :  [0. 1.] [0.         0.23087211]\n",
      "action and advantages :  [1. 0.] [0.1829414 0.       ]\n",
      "action and advantages :  [0. 1.] [0.         0.13849621]\n",
      "action and advantages :  [1. 0.] [0.09711917 0.        ]\n",
      "action and advantages :  [0. 1.] [0.         0.05843102]\n",
      "action and advantages :  [1. 0.] [0.02208662 0.        ]\n",
      "action and advantages :  [0. 1.] [-0.         -0.01222867]\n",
      "action and advantages :  [0. 1.] [-0.         -0.04480226]\n",
      "action and advantages :  [1. 0.] [-0.07589724 -0.        ]\n",
      "action and advantages :  [0. 1.] [-0.         -0.10575511]\n",
      "action and advantages :  [1. 0.] [-0.13459812 -0.        ]\n",
      "action and advantages :  [1. 0.] [-0.16263149 -0.        ]\n",
      "action and advantages :  [0. 1.] [-0.         -0.19004537]\n",
      "action and advantages :  [1. 0.] [-0.21701665 -0.        ]\n",
      "action and advantages :  [1. 0.] [-0.24371058 -0.        ]\n",
      "action and advantages :  [0. 1.] [-0.         -0.27028226]\n",
      "action and advantages :  [1. 0.] [-0.29687797 -0.        ]\n",
      "action and advantages :  [0. 1.] [-0.         -0.32363642]\n",
      "action and advantages :  [0. 1.] [-0.         -0.35068987]\n",
      "action and advantages :  [1. 0.] [-0.3781652 -0.       ]\n",
      "action and advantages :  [1. 0.] [-0.40618481 -0.        ]\n",
      "action and advantages :  [0. 1.] [-0.         -0.43486751]\n",
      "action and advantages :  [1. 0.] [-0.46432937 -0.        ]\n",
      "action and advantages :  [0. 1.] [-0.         -0.49468444]\n",
      "action and advantages :  [0. 1.] [-0.         -0.52604549]\n",
      "action and advantages :  [0. 1.] [-0.         -0.55852461]\n",
      "action and advantages :  [0. 1.] [-0.        -0.5922339]\n",
      "action and advantages :  [0. 1.] [-0.         -0.62728605]\n",
      "action and advantages :  [0. 1.] [-0.         -0.66379485]\n",
      "action and advantages :  [1. 0.] [-0.70187579 -0.        ]\n",
      "action and advantages :  [0. 1.] [-0.         -0.74164656]\n",
      "action and advantages :  [0. 1.] [-0.         -0.78322753]\n",
      "action and advantages :  [0. 1.] [-0.         -0.82674227]\n",
      "action and advantages :  [0. 1.] [-0.         -0.87231805]\n",
      "action and advantages :  [0. 1.] [-0.         -0.92008624]\n",
      "X_batch size :  (49, 80, 80, 4)\n",
      "Y_batch size :  (49, 2)\n",
      "using batch_size =  48\n",
      "creating: createRMSprop\n",
      "creating: createPGCriterion\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 80, 4)\n",
      "out result  [array([0.5034786 , 0.49652138], dtype=float32)]\n",
      "living steps :  2\n",
      "out result  [array([0.5053683 , 0.49463177], dtype=float32)]\n",
      "living steps :  3\n",
      "out result  [array([0.50444   , 0.49555996], dtype=float32)]\n",
      "living steps :  4\n",
      "out result  [array([0.5022209 , 0.49777907], dtype=float32)]\n",
      "living steps :  5\n",
      "out result  [array([0.50218576, 0.49781433], dtype=float32)]\n",
      "living steps :  6\n",
      "out result  [array([0.5006731 , 0.49932688], dtype=float32)]\n",
      "living steps :  7\n",
      "out result  [array([0.49927127, 0.5007288 ], dtype=float32)]\n",
      "living steps :  8\n",
      "out result  [array([0.4986009, 0.5013991], dtype=float32)]\n",
      "living steps :  9\n",
      "out result  [array([0.50129414, 0.49870583], dtype=float32)]\n",
      "living steps :  10\n",
      "out result  [array([0.49886546, 0.5011346 ], dtype=float32)]\n",
      "living steps :  11\n",
      "out result  [array([0.49993104, 0.5000689 ], dtype=float32)]\n",
      "living steps :  12\n",
      "out result  [array([0.50398666, 0.4960133 ], dtype=float32)]\n",
      "living steps :  13\n",
      "out result  [array([0.50609285, 0.49390718], dtype=float32)]\n",
      "living steps :  14\n",
      "out result  [array([0.50567275, 0.49432722], dtype=float32)]\n",
      "living steps :  15\n",
      "out result  [array([0.50773585, 0.4922641 ], dtype=float32)]\n",
      "living steps :  16\n",
      "out result  [array([0.50904953, 0.49095044], dtype=float32)]\n",
      "living steps :  17\n",
      "out result  [array([0.5065359 , 0.49346417], dtype=float32)]\n",
      "living steps :  18\n",
      "out result  [array([0.50656503, 0.49343497], dtype=float32)]\n",
      "living steps :  19\n",
      "out result  [array([0.5075302 , 0.49246973], dtype=float32)]\n",
      "living steps :  20\n",
      "out result  [array([0.5042654 , 0.49573457], dtype=float32)]\n",
      "living steps :  21\n",
      "out result  [array([0.50013506, 0.49986503], dtype=float32)]\n",
      "living steps :  22\n",
      "out result  [array([0.500548  , 0.49945197], dtype=float32)]\n",
      "living steps :  23\n",
      "out result  [array([0.5016323, 0.4983677], dtype=float32)]\n",
      "living steps :  24\n",
      "out result  [array([0.50509506, 0.49490494], dtype=float32)]\n",
      "living steps :  25\n",
      "out result  [array([0.5045971 , 0.49540287], dtype=float32)]\n",
      "living steps :  26\n",
      "out result  [array([0.5030381 , 0.49696186], dtype=float32)]\n",
      "living steps :  27\n",
      "out result  [array([0.49874982, 0.5012502 ], dtype=float32)]\n",
      "living steps :  28\n",
      "out result  [array([0.4997437, 0.5002563], dtype=float32)]\n",
      "living steps :  29\n",
      "out result  [array([0.49925375, 0.5007463 ], dtype=float32)]\n",
      "living steps :  30\n",
      "out result  [array([0.4977676, 0.5022323], dtype=float32)]\n",
      "living steps :  31\n",
      "out result  [array([0.49603182, 0.5039681 ], dtype=float32)]\n",
      "living steps :  32\n",
      "out result  [array([0.49778688, 0.5022132 ], dtype=float32)]\n",
      "living steps :  33\n",
      "out result  [array([0.49874622, 0.5012538 ], dtype=float32)]\n",
      "living steps :  34\n",
      "out result  [array([0.49814466, 0.5018554 ], dtype=float32)]\n",
      "living steps :  35\n",
      "out result  [array([0.498079  , 0.50192094], dtype=float32)]\n",
      "living steps :  36\n",
      "out result  [array([0.499778  , 0.50022197], dtype=float32)]\n",
      "living steps :  37\n",
      "out result  [array([0.50249666, 0.4975034 ], dtype=float32)]\n",
      "living steps :  38\n",
      "out result  [array([0.50441915, 0.49558094], dtype=float32)]\n",
      "living steps :  39\n",
      "out result  [array([0.50684774, 0.49315226], dtype=float32)]\n",
      "living steps :  40\n",
      "out result  [array([0.5044593 , 0.49554065], dtype=float32)]\n",
      "living steps :  41\n",
      "out result  [array([0.50493145, 0.49506855], dtype=float32)]\n",
      "living steps :  42\n",
      "out result  [array([0.50735295, 0.49264708], dtype=float32)]\n",
      "living steps :  43\n",
      "out result  [array([0.51188296, 0.488117  ], dtype=float32)]\n",
      "living steps :  44\n",
      "out result  [array([0.5122423 , 0.48775768], dtype=float32)]\n",
      "living steps :  45\n",
      "out result  [array([0.5147527 , 0.48524734], dtype=float32)]\n",
      "living steps :  46\n",
      "out result  [array([0.5123271, 0.4876729], dtype=float32)]\n",
      "living steps :  47\n",
      "out result  [array([0.51480407, 0.48519593], dtype=float32)]\n",
      "living steps :  48\n",
      "out result  [array([0.5179799 , 0.48202014], dtype=float32)]\n",
      "living steps :  49\n",
      "out result  [array([0.5195827, 0.4804173], dtype=float32)]\n",
      "living steps :  50\n",
      "reward mean 3.8\n",
      "reward std 0.0\n",
      "reward max 3.8\n",
      "used in training:  2 - 3\n",
      "num of total steps in training  49\n",
      "training begin\n",
      "running_rewards and r_reward_moving_average 1.7442272289970748 -0.07991375911470933\n",
      "running_rewards and r_reward_moving_average 1.7307655042074472 0.10250033969646906\n",
      "running_rewards and r_reward_moving_average 1.7165952675867866 0.26532685614756685\n",
      "running_rewards and r_reward_moving_average 1.7016792290387228 0.4104536972914888\n",
      "running_rewards and r_reward_moving_average 1.6859781358302346 0.5395762504662122\n",
      "running_rewards and r_reward_moving_average 1.6694506692949838 0.6542164390026144\n",
      "running_rewards and r_reward_moving_average 1.652053336099983 0.7557398620318514\n",
      "running_rewards and r_reward_moving_average 1.6337403537894557 0.8453712094386645\n",
      "running_rewards and r_reward_moving_average 1.6144635303046901 0.9242081238737436\n",
      "running_rewards and r_reward_moving_average 1.5941721371628317 0.9932336645168383\n",
      "running_rewards and r_reward_moving_average 1.5728127759608754 1.0533275117814376\n",
      "running_rewards and r_reward_moving_average 1.5503292378535531 1.1052760381993814\n",
      "running_rewards and r_reward_moving_average 1.5266623556353192 1.1497813581647987\n",
      "running_rewards and r_reward_moving_average 1.501749848037178 1.1874694579118508\n",
      "running_rewards and r_reward_moving_average 1.4755261558286084 1.2188974969243835\n",
      "running_rewards and r_reward_moving_average 1.4479222692932718 1.244560362814806\n",
      "running_rewards and r_reward_moving_average 1.4188655466244966 1.2648965534626526\n",
      "running_rewards and r_reward_moving_average 1.388279522762628 1.280293452778837\n",
      "running_rewards and r_reward_moving_average 1.3560837081711872 1.2910920597772162\n",
      "running_rewards and r_reward_moving_average 1.3221933770223022 1.2975912246166132\n",
      "running_rewards and r_reward_moving_average 1.2865193442340024 1.3000514398571823\n",
      "running_rewards and r_reward_moving_average 1.2489677307726341 1.2986982302948644\n",
      "running_rewards and r_reward_moving_average 1.2094397166027726 1.2937251803426415\n",
      "running_rewards and r_reward_moving_average 1.1678312806344975 1.2852966339686545\n",
      "running_rewards and r_reward_moving_average 1.1240329269836815 1.2735500986352388\n",
      "running_rewards and r_reward_moving_average 1.077929396824928 1.2585983814700832\n",
      "running_rewards and r_reward_moving_average 1.0293993650788715 1.2405314830055678\n",
      "running_rewards and r_reward_moving_average 0.9783151211356543 1.2194182712128983\n",
      "running_rewards and r_reward_moving_average 0.9245422327743731 1.1953079562051738\n",
      "running_rewards and r_reward_moving_average 0.867939192394077 1.1682313838620937\n",
      "running_rewards and r_reward_moving_average 0.8083570446253443 1.138202164715292\n",
      "running_rewards and r_reward_moving_average 0.7456389943424677 1.1052176527062971\n",
      "running_rewards and r_reward_moving_average 0.679619994044703 1.069259786869914\n",
      "running_rewards and r_reward_moving_average 0.61012630952074 1.030295807587393\n",
      "running_rewards and r_reward_moving_average 0.5369750626534106 0.9882788577807277\n",
      "running_rewards and r_reward_moving_average 0.4599737501614848 0.943148478267996\n",
      "running_rewards and r_reward_moving_average 0.37891973701208925 0.8948310054573448\n",
      "running_rewards and r_reward_moving_average 0.2935997231706202 0.8432398786128192\n",
      "running_rewards and r_reward_moving_average 0.20378918228486337 0.7882758630685993\n",
      "running_rewards and r_reward_moving_average 0.10925177082617196 0.7298271949902257\n",
      "running_rewards and r_reward_moving_average 0.00973870613281258 0.6677696525738204\n",
      "running_rewards and r_reward_moving_average -0.09501188828124993 0.6019665579297196\n",
      "running_rewards and r_reward_moving_average -0.20527567187499993 0.5322687133086227\n",
      "running_rewards and r_reward_moving_average -0.32134281249999996 0.45851427479026047\n",
      "running_rewards and r_reward_moving_average -0.44351874999999996 0.3805285660612344\n",
      "running_rewards and r_reward_moving_average -0.572125 0.298123834455111\n",
      "running_rewards and r_reward_moving_average -0.7075 0.2110989510095999\n",
      "running_rewards and r_reward_moving_average -0.85 0.11923905590863995\n",
      "running_rewards and r_reward_moving_average -1.0 0.022315150317775978\n",
      "advantage  [ 1.64172689  1.46543865  1.30614157  1.16210298  1.0317617   0.91371081\n",
      "  0.80668213  0.70953223  0.62122987  0.54084463  0.46753674  0.40054788\n",
      "  0.3391929   0.28285235  0.23096579  0.18302572  0.13857209  0.09718746\n",
      "  0.05849248  0.02214194 -0.01217889 -0.04475745 -0.07585692 -0.10571882\n",
      " -0.13456545 -0.16260209 -0.19001891 -0.21699284 -0.24368915 -0.27026297\n",
      " -0.29686061 -0.32362079 -0.35067581 -0.37815255 -0.40617342 -0.43485726\n",
      " -0.46432014 -0.49467614 -0.52603801 -0.55851788 -0.59222785 -0.6272806\n",
      " -0.66378995 -0.70187138 -0.74164258 -0.78322395 -0.82673906 -0.87231515\n",
      " -0.92008364]\n",
      "action and advantages :  [0. 1.] [0.         1.64172689]\n",
      "action and advantages :  [1. 0.] [1.46543865 0.        ]\n",
      "action and advantages :  [0. 1.] [0.         1.30614157]\n",
      "action and advantages :  [1. 0.] [1.16210298 0.        ]\n",
      "action and advantages :  [1. 0.] [1.0317617 0.       ]\n",
      "action and advantages :  [0. 1.] [0.         0.91371081]\n",
      "action and advantages :  [0. 1.] [0.         0.80668213]\n",
      "action and advantages :  [1. 0.] [0.70953223 0.        ]\n",
      "action and advantages :  [0. 1.] [0.         0.62122987]\n",
      "action and advantages :  [0. 1.] [0.         0.54084463]\n",
      "action and advantages :  [1. 0.] [0.46753674 0.        ]\n",
      "action and advantages :  [0. 1.] [0.         0.40054788]\n",
      "action and advantages :  [0. 1.] [0.        0.3391929]\n",
      "action and advantages :  [0. 1.] [0.         0.28285235]\n",
      "action and advantages :  [0. 1.] [0.         0.23096579]\n",
      "action and advantages :  [1. 0.] [0.18302572 0.        ]\n",
      "action and advantages :  [1. 0.] [0.13857209 0.        ]\n",
      "action and advantages :  [1. 0.] [0.09718746 0.        ]\n",
      "action and advantages :  [1. 0.] [0.05849248 0.        ]\n",
      "action and advantages :  [0. 1.] [0.         0.02214194]\n",
      "action and advantages :  [0. 1.] [-0.         -0.01217889]\n",
      "action and advantages :  [0. 1.] [-0.         -0.04475745]\n",
      "action and advantages :  [0. 1.] [-0.         -0.07585692]\n",
      "action and advantages :  [0. 1.] [-0.         -0.10571882]\n",
      "action and advantages :  [0. 1.] [-0.         -0.13456545]\n",
      "action and advantages :  [1. 0.] [-0.16260209 -0.        ]\n",
      "action and advantages :  [0. 1.] [-0.         -0.19001891]\n",
      "action and advantages :  [0. 1.] [-0.         -0.21699284]\n",
      "action and advantages :  [0. 1.] [-0.         -0.24368915]\n",
      "action and advantages :  [1. 0.] [-0.27026297 -0.        ]\n",
      "action and advantages :  [1. 0.] [-0.29686061 -0.        ]\n",
      "action and advantages :  [1. 0.] [-0.32362079 -0.        ]\n",
      "action and advantages :  [0. 1.] [-0.         -0.35067581]\n",
      "action and advantages :  [0. 1.] [-0.         -0.37815255]\n",
      "action and advantages :  [0. 1.] [-0.         -0.40617342]\n",
      "action and advantages :  [0. 1.] [-0.         -0.43485726]\n",
      "action and advantages :  [1. 0.] [-0.46432014 -0.        ]\n",
      "action and advantages :  [1. 0.] [-0.49467614 -0.        ]\n",
      "action and advantages :  [1. 0.] [-0.52603801 -0.        ]\n",
      "action and advantages :  [1. 0.] [-0.55851788 -0.        ]\n",
      "action and advantages :  [0. 1.] [-0.         -0.59222785]\n",
      "action and advantages :  [1. 0.] [-0.6272806 -0.       ]\n",
      "action and advantages :  [0. 1.] [-0.         -0.66378995]\n",
      "action and advantages :  [1. 0.] [-0.70187138 -0.        ]\n",
      "action and advantages :  [0. 1.] [-0.         -0.74164258]\n",
      "action and advantages :  [0. 1.] [-0.         -0.78322395]\n",
      "action and advantages :  [0. 1.] [-0.         -0.82673906]\n",
      "action and advantages :  [1. 0.] [-0.87231515 -0.        ]\n",
      "action and advantages :  [0. 1.] [-0.         -0.92008364]\n",
      "X_batch size :  (49, 80, 80, 4)\n",
      "Y_batch size :  (49, 2)\n",
      "using batch_size =  48\n",
      "creating: createRMSprop\n",
      "creating: createPGCriterion\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 80, 4)\n",
      "out result  [array([0.50390625, 0.49609375], dtype=float32)]\n",
      "living steps :  2\n",
      "out result  [array([0.5062046 , 0.49379537], dtype=float32)]\n",
      "living steps :  3\n",
      "out result  [array([0.5052547 , 0.49474534], dtype=float32)]\n",
      "living steps :  4\n",
      "out result  [array([0.5041525, 0.4958475], dtype=float32)]\n",
      "living steps :  5\n",
      "out result  [array([0.5037933 , 0.49620673], dtype=float32)]\n",
      "living steps :  6\n",
      "out result  [array([0.5020191, 0.4979809], dtype=float32)]\n",
      "living steps :  7\n",
      "out result  [array([0.50060266, 0.49939734], dtype=float32)]\n",
      "living steps :  8\n",
      "out result  [array([0.4996656 , 0.50033444], dtype=float32)]\n",
      "living steps :  9\n",
      "out result  [array([0.50206023, 0.49793983], dtype=float32)]\n",
      "living steps :  10\n",
      "out result  [array([0.5013234 , 0.49867663], dtype=float32)]\n",
      "living steps :  11\n",
      "out result  [array([0.50125307, 0.49874702], dtype=float32)]\n",
      "living steps :  12\n",
      "out result  [array([0.50527054, 0.49472946], dtype=float32)]\n",
      "living steps :  13\n",
      "out result  [array([0.5077537, 0.4922463], dtype=float32)]\n",
      "living steps :  14\n",
      "out result  [array([0.5081017 , 0.49189833], dtype=float32)]\n",
      "living steps :  15\n",
      "out result  [array([0.5103134 , 0.48968658], dtype=float32)]\n",
      "living steps :  16\n",
      "out result  [array([0.5105935 , 0.48940644], dtype=float32)]\n",
      "living steps :  17\n",
      "out result  [array([0.5097497 , 0.49025038], dtype=float32)]\n",
      "living steps :  18\n",
      "out result  [array([0.5089421, 0.4910579], dtype=float32)]\n",
      "living steps :  19\n",
      "out result  [array([0.5096074, 0.4903926], dtype=float32)]\n",
      "living steps :  20\n",
      "out result  [array([0.5070939 , 0.49290612], dtype=float32)]\n",
      "living steps :  21\n",
      "out result  [array([0.50288177, 0.49711826], dtype=float32)]\n",
      "living steps :  22\n",
      "out result  [array([0.5026537 , 0.49734625], dtype=float32)]\n",
      "living steps :  23\n",
      "out result  [array([0.50523, 0.49477], dtype=float32)]\n",
      "living steps :  24\n",
      "out result  [array([0.5075176 , 0.49248245], dtype=float32)]\n",
      "living steps :  25\n",
      "out result  [array([0.5073786 , 0.49262145], dtype=float32)]\n",
      "living steps :  26\n",
      "out result  [array([0.5065268 , 0.49347314], dtype=float32)]\n",
      "living steps :  27\n",
      "out result  [array([0.5013943, 0.4986058], dtype=float32)]\n",
      "living steps :  28\n",
      "out result  [array([0.50142777, 0.49857226], dtype=float32)]\n",
      "living steps :  29\n",
      "out result  [array([0.50230044, 0.49769953], dtype=float32)]\n",
      "living steps :  30\n",
      "out result  [array([0.4993323 , 0.50066763], dtype=float32)]\n",
      "living steps :  31\n",
      "out result  [array([0.49535298, 0.50464696], dtype=float32)]\n",
      "living steps :  32\n",
      "out result  [array([0.49767834, 0.5023217 ], dtype=float32)]\n",
      "living steps :  33\n",
      "out result  [array([0.5005854 , 0.49941456], dtype=float32)]\n",
      "living steps :  34\n",
      "out result  [array([0.5007092 , 0.49929088], dtype=float32)]\n",
      "living steps :  35\n",
      "out result  [array([0.5009921 , 0.49900788], dtype=float32)]\n",
      "living steps :  36\n",
      "out result  [array([0.5017937 , 0.49820635], dtype=float32)]\n",
      "living steps :  37\n",
      "out result  [array([0.50496775, 0.49503222], dtype=float32)]\n",
      "living steps :  38\n",
      "out result  [array([0.50654024, 0.49345982], dtype=float32)]\n",
      "living steps :  39\n",
      "out result  [array([0.5109288, 0.4890712], dtype=float32)]\n",
      "living steps :  40\n",
      "out result  [array([0.5086576 , 0.49134246], dtype=float32)]\n",
      "living steps :  41\n",
      "out result  [array([0.5087192 , 0.49128082], dtype=float32)]\n",
      "living steps :  42\n",
      "out result  [array([0.5101291 , 0.48987085], dtype=float32)]\n",
      "living steps :  43\n",
      "out result  [array([0.5136929 , 0.48630708], dtype=float32)]\n",
      "living steps :  44\n",
      "out result  [array([0.5134468 , 0.48655316], dtype=float32)]\n",
      "living steps :  45\n",
      "out result  [array([0.51646364, 0.4835364 ], dtype=float32)]\n",
      "living steps :  46\n",
      "out result  [array([0.5146183 , 0.48538175], dtype=float32)]\n",
      "living steps :  47\n",
      "out result  [array([0.5147258 , 0.48527426], dtype=float32)]\n",
      "living steps :  48\n",
      "out result  [array([0.51750934, 0.48249066], dtype=float32)]\n",
      "living steps :  49\n",
      "out result  [array([0.5211194 , 0.47888058], dtype=float32)]\n",
      "living steps :  50\n",
      "reward mean 3.8\n",
      "reward std 0.0\n",
      "reward max 3.8\n",
      "used in training:  3 - 4\n",
      "num of total steps in training  49\n",
      "training begin\n",
      "running_rewards and r_reward_moving_average 1.7442272289970748 -0.0799163647140016\n",
      "running_rewards and r_reward_moving_average 1.7307655042074472 0.102497994657106\n",
      "running_rewards and r_reward_moving_average 1.7165952675867866 0.2653247456121401\n",
      "running_rewards and r_reward_moving_average 1.7016792290387228 0.4104517978096047\n",
      "running_rewards and r_reward_moving_average 1.6859781358302346 0.5395745409325166\n",
      "running_rewards and r_reward_moving_average 1.6694506692949838 0.6542149004222884\n",
      "running_rewards and r_reward_moving_average 1.652053336099983 0.7557384773095579\n",
      "running_rewards and r_reward_moving_average 1.6337403537894557 0.8453699631886004\n",
      "running_rewards and r_reward_moving_average 1.6144635303046901 0.9242070022486859\n",
      "running_rewards and r_reward_moving_average 1.5941721371628317 0.9932326550542863\n",
      "running_rewards and r_reward_moving_average 1.5728127759608754 1.0533266032651407\n",
      "running_rewards and r_reward_moving_average 1.5503292378535531 1.1052752205347143\n",
      "running_rewards and r_reward_moving_average 1.5266623556353192 1.149780622266598\n",
      "running_rewards and r_reward_moving_average 1.501749848037178 1.1874687956034702\n",
      "running_rewards and r_reward_moving_average 1.4755261558286084 1.2188969008468409\n",
      "running_rewards and r_reward_moving_average 1.4479222692932718 1.2445598263450177\n",
      "running_rewards and r_reward_moving_average 1.4188655466244966 1.264896070639843\n",
      "running_rewards and r_reward_moving_average 1.388279522762628 1.2802930182383085\n",
      "running_rewards and r_reward_moving_average 1.3560837081711872 1.2910916686907405\n",
      "running_rewards and r_reward_moving_average 1.3221933770223022 1.2975908726387853\n",
      "running_rewards and r_reward_moving_average 1.2865193442340024 1.300051123077137\n",
      "running_rewards and r_reward_moving_average 1.2489677307726341 1.2986979451928236\n",
      "running_rewards and r_reward_moving_average 1.2094397166027726 1.2937249237508046\n",
      "running_rewards and r_reward_moving_average 1.1678312806344975 1.2852964030360012\n",
      "running_rewards and r_reward_moving_average 1.1240329269836815 1.273549890795851\n",
      "running_rewards and r_reward_moving_average 1.077929396824928 1.2585981944146338\n",
      "running_rewards and r_reward_moving_average 1.0293993650788715 1.2405313146556634\n",
      "running_rewards and r_reward_moving_average 0.9783151211356543 1.219418119697984\n",
      "running_rewards and r_reward_moving_average 0.9245422327743731 1.195307819841751\n",
      "running_rewards and r_reward_moving_average 0.867939192394077 1.1682312611350132\n",
      "running_rewards and r_reward_moving_average 0.8083570446253443 1.1382020542609195\n",
      "running_rewards and r_reward_moving_average 0.7456389943424677 1.1052175532973618\n",
      "running_rewards and r_reward_moving_average 0.679619994044703 1.0692596974018724\n",
      "running_rewards and r_reward_moving_average 0.61012630952074 1.0302957270661555\n",
      "running_rewards and r_reward_moving_average 0.5369750626534106 0.988278785311614\n",
      "running_rewards and r_reward_moving_average 0.4599737501614848 0.9431484130457937\n",
      "running_rewards and r_reward_moving_average 0.37891973701208925 0.8948309467573627\n",
      "running_rewards and r_reward_moving_average 0.2935997231706202 0.8432398257828354\n",
      "running_rewards and r_reward_moving_average 0.20378918228486337 0.7882758155216139\n",
      "running_rewards and r_reward_moving_average 0.10925177082617196 0.7298271521979388\n",
      "running_rewards and r_reward_moving_average 0.00973870613281258 0.6677696140607622\n",
      "running_rewards and r_reward_moving_average -0.09501188828124993 0.6019665232679672\n",
      "running_rewards and r_reward_moving_average -0.20527567187499993 0.5322686821130455\n",
      "running_rewards and r_reward_moving_average -0.32134281249999996 0.45851424671424096\n",
      "running_rewards and r_reward_moving_average -0.44351874999999996 0.38052854079281684\n",
      "running_rewards and r_reward_moving_average -0.572125 0.2981238117135352\n",
      "running_rewards and r_reward_moving_average -0.7075 0.2110989305421817\n",
      "running_rewards and r_reward_moving_average -0.85 0.11923903748796355\n",
      "running_rewards and r_reward_moving_average -1.0 0.022315133739167214\n",
      "advantage  [ 1.64172923  1.46544076  1.30614347  1.16210469  1.03176324  0.91371219\n",
      "  0.80668337  0.70953335  0.62123088  0.54084553  0.46753756  0.40054862\n",
      "  0.33919356  0.28285295  0.23096633  0.1830262   0.13857253  0.09718785\n",
      "  0.05849284  0.02214225 -0.0121786  -0.04475719 -0.07585669 -0.10571861\n",
      " -0.13456527 -0.16260192 -0.19001875 -0.2169927  -0.24368903 -0.27026286\n",
      " -0.29686051 -0.3236207  -0.35067573 -0.37815248 -0.40617335 -0.4348572\n",
      " -0.46432009 -0.49467609 -0.52603797 -0.55851784 -0.59222782 -0.62728057\n",
      " -0.66378992 -0.70187135 -0.74164256 -0.78322393 -0.82673904 -0.87231513\n",
      " -0.92008362]\n",
      "action and advantages :  [0. 1.] [0.         1.64172923]\n",
      "action and advantages :  [0. 1.] [0.         1.46544076]\n",
      "action and advantages :  [1. 0.] [1.30614347 0.        ]\n",
      "action and advantages :  [0. 1.] [0.         1.16210469]\n",
      "action and advantages :  [1. 0.] [1.03176324 0.        ]\n",
      "action and advantages :  [1. 0.] [0.91371219 0.        ]\n",
      "action and advantages :  [1. 0.] [0.80668337 0.        ]\n",
      "action and advantages :  [0. 1.] [0.         0.70953335]\n",
      "action and advantages :  [0. 1.] [0.         0.62123088]\n",
      "action and advantages :  [0. 1.] [0.         0.54084553]\n",
      "action and advantages :  [0. 1.] [0.         0.46753756]\n",
      "action and advantages :  [0. 1.] [0.         0.40054862]\n",
      "action and advantages :  [1. 0.] [0.33919356 0.        ]\n",
      "action and advantages :  [0. 1.] [0.         0.28285295]\n",
      "action and advantages :  [0. 1.] [0.         0.23096633]\n",
      "action and advantages :  [1. 0.] [0.1830262 0.       ]\n",
      "action and advantages :  [1. 0.] [0.13857253 0.        ]\n",
      "action and advantages :  [1. 0.] [0.09718785 0.        ]\n",
      "action and advantages :  [1. 0.] [0.05849284 0.        ]\n",
      "action and advantages :  [1. 0.] [0.02214225 0.        ]\n",
      "action and advantages :  [0. 1.] [-0.        -0.0121786]\n",
      "action and advantages :  [0. 1.] [-0.         -0.04475719]\n",
      "action and advantages :  [0. 1.] [-0.         -0.07585669]\n",
      "action and advantages :  [0. 1.] [-0.         -0.10571861]\n",
      "action and advantages :  [0. 1.] [-0.         -0.13456527]\n",
      "action and advantages :  [1. 0.] [-0.16260192 -0.        ]\n",
      "action and advantages :  [0. 1.] [-0.         -0.19001875]\n",
      "action and advantages :  [0. 1.] [-0.        -0.2169927]\n",
      "action and advantages :  [1. 0.] [-0.24368903 -0.        ]\n",
      "action and advantages :  [0. 1.] [-0.         -0.27026286]\n",
      "action and advantages :  [0. 1.] [-0.         -0.29686051]\n",
      "action and advantages :  [0. 1.] [-0.        -0.3236207]\n",
      "action and advantages :  [0. 1.] [-0.         -0.35067573]\n",
      "action and advantages :  [1. 0.] [-0.37815248 -0.        ]\n",
      "action and advantages :  [0. 1.] [-0.         -0.40617335]\n",
      "action and advantages :  [1. 0.] [-0.4348572 -0.       ]\n",
      "action and advantages :  [0. 1.] [-0.         -0.46432009]\n",
      "action and advantages :  [0. 1.] [-0.         -0.49467609]\n",
      "action and advantages :  [0. 1.] [-0.         -0.52603797]\n",
      "action and advantages :  [0. 1.] [-0.         -0.55851784]\n",
      "action and advantages :  [1. 0.] [-0.59222782 -0.        ]\n",
      "action and advantages :  [0. 1.] [-0.         -0.62728057]\n",
      "action and advantages :  [0. 1.] [-0.         -0.66378992]\n",
      "action and advantages :  [1. 0.] [-0.70187135 -0.        ]\n",
      "action and advantages :  [1. 0.] [-0.74164256 -0.        ]\n",
      "action and advantages :  [0. 1.] [-0.         -0.78322393]\n",
      "action and advantages :  [0. 1.] [-0.         -0.82673904]\n",
      "action and advantages :  [0. 1.] [-0.         -0.87231513]\n",
      "action and advantages :  [1. 0.] [-0.92008362 -0.        ]\n",
      "X_batch size :  (49, 80, 80, 4)\n",
      "Y_batch size :  (49, 2)\n",
      "using batch_size =  48\n",
      "creating: createRMSprop\n",
      "creating: createPGCriterion\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 80, 4)\n",
      "out result  [array([0.50034   , 0.49966004], dtype=float32)]\n",
      "living steps :  2\n",
      "out result  [array([0.5027504 , 0.49724957], dtype=float32)]\n",
      "living steps :  3\n",
      "out result  [array([0.50298226, 0.4970177 ], dtype=float32)]\n",
      "living steps :  4\n",
      "out result  [array([0.5015437 , 0.49845633], dtype=float32)]\n",
      "living steps :  5\n",
      "out result  [array([0.50263125, 0.49736875], dtype=float32)]\n",
      "living steps :  6\n",
      "out result  [array([0.50171894, 0.49828112], dtype=float32)]\n",
      "living steps :  7\n",
      "out result  [array([0.5005702, 0.4994298], dtype=float32)]\n",
      "living steps :  8\n",
      "out result  [array([0.49901104, 0.500989  ], dtype=float32)]\n",
      "living steps :  9\n",
      "out result  [array([0.5014544 , 0.49854562], dtype=float32)]\n",
      "living steps :  10\n",
      "out result  [array([0.5010381, 0.498962 ], dtype=float32)]\n",
      "living steps :  11\n",
      "out result  [array([0.5017455 , 0.49825445], dtype=float32)]\n",
      "living steps :  12\n",
      "out result  [array([0.503866  , 0.49613398], dtype=float32)]\n",
      "living steps :  13\n",
      "out result  [array([0.5067475 , 0.49325246], dtype=float32)]\n",
      "living steps :  14\n",
      "out result  [array([0.50519216, 0.4948078 ], dtype=float32)]\n",
      "living steps :  15\n",
      "out result  [array([0.50818324, 0.4918168 ], dtype=float32)]\n",
      "living steps :  16\n",
      "out result  [array([0.5068957 , 0.49310428], dtype=float32)]\n",
      "living steps :  17\n",
      "out result  [array([0.50482386, 0.49517617], dtype=float32)]\n",
      "living steps :  18\n",
      "out result  [array([0.504582, 0.495418], dtype=float32)]\n",
      "living steps :  19\n",
      "out result  [array([0.5069792 , 0.49302074], dtype=float32)]\n",
      "living steps :  20\n",
      "out result  [array([0.5049054 , 0.49509466], dtype=float32)]\n",
      "living steps :  21\n",
      "out result  [array([0.5022851, 0.4977148], dtype=float32)]\n",
      "living steps :  22\n",
      "out result  [array([0.50278455, 0.49721548], dtype=float32)]\n",
      "living steps :  23\n",
      "out result  [array([0.5056716 , 0.49432832], dtype=float32)]\n",
      "living steps :  24\n",
      "out result  [array([0.50644946, 0.49355048], dtype=float32)]\n",
      "living steps :  25\n",
      "out result  [array([0.50679415, 0.49320582], dtype=float32)]\n",
      "living steps :  26\n",
      "out result  [array([0.50602734, 0.4939726 ], dtype=float32)]\n",
      "living steps :  27\n",
      "out result  [array([0.5040089 , 0.49599105], dtype=float32)]\n",
      "living steps :  28\n",
      "out result  [array([0.5035013, 0.4964986], dtype=float32)]\n",
      "living steps :  29\n",
      "out result  [array([0.5031376 , 0.49686238], dtype=float32)]\n",
      "living steps :  30\n",
      "out result  [array([0.5009196, 0.4990804], dtype=float32)]\n",
      "living steps :  31\n",
      "out result  [array([0.495825, 0.504175], dtype=float32)]\n",
      "living steps :  32\n",
      "out result  [array([0.49595535, 0.5040447 ], dtype=float32)]\n",
      "living steps :  33\n",
      "out result  [array([0.49835584, 0.50164413], dtype=float32)]\n",
      "living steps :  34\n",
      "out result  [array([0.49892205, 0.5010779 ], dtype=float32)]\n",
      "living steps :  35\n",
      "out result  [array([0.49927482, 0.50072527], dtype=float32)]\n",
      "living steps :  36\n",
      "out result  [array([0.5000283, 0.4999717], dtype=float32)]\n",
      "living steps :  37\n",
      "out result  [array([0.5040499 , 0.49595007], dtype=float32)]\n",
      "living steps :  38\n",
      "out result  [array([0.50645906, 0.4935409 ], dtype=float32)]\n",
      "living steps :  39\n",
      "out result  [array([0.5073972, 0.4926028], dtype=float32)]\n",
      "living steps :  40\n",
      "out result  [array([0.5077799 , 0.49222013], dtype=float32)]\n",
      "living steps :  41\n",
      "out result  [array([0.50823367, 0.49176633], dtype=float32)]\n",
      "living steps :  42\n",
      "out result  [array([0.5119124 , 0.48808756], dtype=float32)]\n",
      "living steps :  43\n",
      "out result  [array([0.5177818, 0.4822182], dtype=float32)]\n",
      "living steps :  44\n",
      "out result  [array([0.5191442 , 0.48085582], dtype=float32)]\n",
      "living steps :  45\n",
      "out result  [array([0.52200514, 0.47799486], dtype=float32)]\n",
      "living steps :  46\n",
      "out result  [array([0.52037174, 0.47962824], dtype=float32)]\n",
      "living steps :  47\n",
      "out result  [array([0.5187342 , 0.48126572], dtype=float32)]\n",
      "living steps :  48\n",
      "out result  [array([0.51698977, 0.4830102 ], dtype=float32)]\n",
      "living steps :  49\n",
      "out result  [array([0.51585406, 0.484146  ], dtype=float32)]\n",
      "living steps :  50\n",
      "reward mean 3.8\n",
      "reward std 0.0\n",
      "reward max 3.8\n",
      "used in training:  4 - 5\n",
      "num of total steps in training  49\n",
      "training begin\n",
      "running_rewards and r_reward_moving_average 1.7442272289970748 -0.07991637963474948\n",
      "running_rewards and r_reward_moving_average 1.7307655042074472 0.10249798122843291\n",
      "running_rewards and r_reward_moving_average 1.7165952675867866 0.2653247335263343\n",
      "running_rewards and r_reward_moving_average 1.7016792290387228 0.4104517869323795\n",
      "running_rewards and r_reward_moving_average 1.6859781358302346 0.5395745311430138\n",
      "running_rewards and r_reward_moving_average 1.6694506692949838 0.6542148916117358\n",
      "running_rewards and r_reward_moving_average 1.652053336099983 0.7557384693800606\n",
      "running_rewards and r_reward_moving_average 1.6337403537894557 0.8453699560520528\n",
      "running_rewards and r_reward_moving_average 1.6144635303046901 0.9242069958257931\n",
      "running_rewards and r_reward_moving_average 1.5941721371628317 0.9932326492736828\n",
      "running_rewards and r_reward_moving_average 1.5728127759608754 1.0533265980625977\n",
      "running_rewards and r_reward_moving_average 1.5503292378535531 1.1052752158524255\n",
      "running_rewards and r_reward_moving_average 1.5266623556353192 1.1497806180525383\n",
      "running_rewards and r_reward_moving_average 1.501749848037178 1.1874687918108164\n",
      "running_rewards and r_reward_moving_average 1.4755261558286084 1.2188968974334526\n",
      "running_rewards and r_reward_moving_average 1.4479222692932718 1.2445598232729682\n",
      "running_rewards and r_reward_moving_average 1.4188655466244966 1.2648960678749985\n",
      "running_rewards and r_reward_moving_average 1.388279522762628 1.2802930157499484\n",
      "running_rewards and r_reward_moving_average 1.3560837081711872 1.2910916664512164\n",
      "running_rewards and r_reward_moving_average 1.3221933770223022 1.2975908706232135\n",
      "running_rewards and r_reward_moving_average 1.2865193442340024 1.3000511212631225\n",
      "running_rewards and r_reward_moving_average 1.2489677307726341 1.2986979435602106\n",
      "running_rewards and r_reward_moving_average 1.2094397166027726 1.293724922281453\n",
      "running_rewards and r_reward_moving_average 1.1678312806344975 1.285296401713585\n",
      "running_rewards and r_reward_moving_average 1.1240329269836815 1.2735498896056763\n",
      "running_rewards and r_reward_moving_average 1.077929396824928 1.258598193343477\n",
      "running_rewards and r_reward_moving_average 1.0293993650788715 1.240531313691622\n",
      "running_rewards and r_reward_moving_average 0.9783151211356543 1.219418118830347\n",
      "running_rewards and r_reward_moving_average 0.9245422327743731 1.1953078190608777\n",
      "running_rewards and r_reward_moving_average 0.867939192394077 1.1682312604322271\n",
      "running_rewards and r_reward_moving_average 0.8083570446253443 1.138202053628412\n",
      "running_rewards and r_reward_moving_average 0.7456389943424677 1.1052175527281052\n",
      "running_rewards and r_reward_moving_average 0.679619994044703 1.0692596968895414\n",
      "running_rewards and r_reward_moving_average 0.61012630952074 1.0302957266050576\n",
      "running_rewards and r_reward_moving_average 0.5369750626534106 0.9882787848966259\n",
      "running_rewards and r_reward_moving_average 0.4599737501614848 0.9431484126723044\n",
      "running_rewards and r_reward_moving_average 0.37891973701208925 0.8948309464212224\n",
      "running_rewards and r_reward_moving_average 0.2935997231706202 0.8432398254803091\n",
      "running_rewards and r_reward_moving_average 0.20378918228486337 0.7882758152493402\n",
      "running_rewards and r_reward_moving_average 0.10925177082617196 0.7298271519528926\n",
      "running_rewards and r_reward_moving_average 0.00973870613281258 0.6677696138402206\n",
      "running_rewards and r_reward_moving_average -0.09501188828124993 0.6019665230694798\n",
      "running_rewards and r_reward_moving_average -0.20527567187499993 0.5322686819344069\n",
      "running_rewards and r_reward_moving_average -0.32134281249999996 0.45851424655346623\n",
      "running_rewards and r_reward_moving_average -0.44351874999999996 0.3805285406481196\n",
      "running_rewards and r_reward_moving_average -0.572125 0.29812381158330764\n",
      "running_rewards and r_reward_moving_average -0.7075 0.2110989304249769\n",
      "running_rewards and r_reward_moving_average -0.85 0.11923903738247923\n",
      "running_rewards and r_reward_moving_average -1.0 0.022315133644231336\n",
      "advantage  [ 1.64172925  1.46544077  1.30614348  1.1621047   1.03176324  0.9137122\n",
      "  0.80668338  0.70953336  0.62123088  0.54084554  0.46753756  0.40054862\n",
      "  0.33919356  0.28285295  0.23096633  0.1830262   0.13857253  0.09718786\n",
      "  0.05849284  0.02214226 -0.0121786  -0.04475719 -0.07585669 -0.10571861\n",
      " -0.13456527 -0.16260192 -0.19001875 -0.2169927  -0.24368903 -0.27026286\n",
      " -0.29686051 -0.3236207  -0.35067573 -0.37815248 -0.40617335 -0.4348572\n",
      " -0.46432009 -0.49467609 -0.52603797 -0.55851784 -0.59222782 -0.62728057\n",
      " -0.66378992 -0.70187135 -0.74164256 -0.78322393 -0.82673904 -0.87231513\n",
      " -0.92008362]\n",
      "action and advantages :  [0. 1.] [0.         1.64172925]\n",
      "action and advantages :  [1. 0.] [1.46544077 0.        ]\n",
      "action and advantages :  [0. 1.] [0.         1.30614348]\n",
      "action and advantages :  [0. 1.] [0.        1.1621047]\n",
      "action and advantages :  [1. 0.] [1.03176324 0.        ]\n",
      "action and advantages :  [1. 0.] [0.9137122 0.       ]\n",
      "action and advantages :  [1. 0.] [0.80668338 0.        ]\n",
      "action and advantages :  [0. 1.] [0.         0.70953336]\n",
      "action and advantages :  [1. 0.] [0.62123088 0.        ]\n",
      "action and advantages :  [0. 1.] [0.         0.54084554]\n",
      "action and advantages :  [0. 1.] [0.         0.46753756]\n",
      "action and advantages :  [1. 0.] [0.40054862 0.        ]\n",
      "action and advantages :  [1. 0.] [0.33919356 0.        ]\n",
      "action and advantages :  [0. 1.] [0.         0.28285295]\n",
      "action and advantages :  [0. 1.] [0.         0.23096633]\n",
      "action and advantages :  [0. 1.] [0.        0.1830262]\n",
      "action and advantages :  [1. 0.] [0.13857253 0.        ]\n",
      "action and advantages :  [1. 0.] [0.09718786 0.        ]\n",
      "action and advantages :  [0. 1.] [0.         0.05849284]\n",
      "action and advantages :  [0. 1.] [0.         0.02214226]\n",
      "action and advantages :  [0. 1.] [-0.        -0.0121786]\n",
      "action and advantages :  [0. 1.] [-0.         -0.04475719]\n",
      "action and advantages :  [1. 0.] [-0.07585669 -0.        ]\n",
      "action and advantages :  [1. 0.] [-0.10571861 -0.        ]\n",
      "action and advantages :  [0. 1.] [-0.         -0.13456527]\n",
      "action and advantages :  [0. 1.] [-0.         -0.16260192]\n",
      "action and advantages :  [1. 0.] [-0.19001875 -0.        ]\n",
      "action and advantages :  [0. 1.] [-0.        -0.2169927]\n",
      "action and advantages :  [0. 1.] [-0.         -0.24368903]\n",
      "action and advantages :  [0. 1.] [-0.         -0.27026286]\n",
      "action and advantages :  [0. 1.] [-0.         -0.29686051]\n",
      "action and advantages :  [0. 1.] [-0.        -0.3236207]\n",
      "action and advantages :  [0. 1.] [-0.         -0.35067573]\n",
      "action and advantages :  [0. 1.] [-0.         -0.37815248]\n",
      "action and advantages :  [0. 1.] [-0.         -0.40617335]\n",
      "action and advantages :  [1. 0.] [-0.4348572 -0.       ]\n",
      "action and advantages :  [1. 0.] [-0.46432009 -0.        ]\n",
      "action and advantages :  [1. 0.] [-0.49467609 -0.        ]\n",
      "action and advantages :  [1. 0.] [-0.52603797 -0.        ]\n",
      "action and advantages :  [0. 1.] [-0.         -0.55851784]\n",
      "action and advantages :  [1. 0.] [-0.59222782 -0.        ]\n",
      "action and advantages :  [1. 0.] [-0.62728057 -0.        ]\n",
      "action and advantages :  [1. 0.] [-0.66378992 -0.        ]\n",
      "action and advantages :  [1. 0.] [-0.70187135 -0.        ]\n",
      "action and advantages :  [0. 1.] [-0.         -0.74164256]\n",
      "action and advantages :  [1. 0.] [-0.78322393 -0.        ]\n",
      "action and advantages :  [1. 0.] [-0.82673904 -0.        ]\n",
      "action and advantages :  [0. 1.] [-0.         -0.87231513]\n",
      "action and advantages :  [0. 1.] [-0.         -0.92008362]\n",
      "X_batch size :  (49, 80, 80, 4)\n",
      "Y_batch size :  (49, 2)\n",
      "using batch_size =  48\n",
      "creating: createRMSprop\n",
      "creating: createPGCriterion\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "state_size = (1, 80, 80, 4)\n",
    "action_size = ACTION_SIZE\n",
    "agent = BirdAgent(state_size, action_size)\n",
    "history = ExperienceStroe()\n",
    "record = []\n",
    "exe_times = []\n",
    "history.reset()\n",
    "train_start = 0\n",
    "played_steps = 0\n",
    "start_of_play = timeit.default_timer()\n",
    "for i in range(5):\n",
    "    # history is a list of rollouts\n",
    "    start_eps, end_eps, steps = play_n_games(agent, history, n=BATCH_SIZE)\n",
    "    played_steps += steps\n",
    "    train_end = end_eps\n",
    "    end_of_play = timeit.default_timer()\n",
    "    print\n",
    "    \"*************************\"\n",
    "    rollouts = history.get_range(start_eps, end_eps)\n",
    "    stats_summary(rollouts, record)\n",
    "    if (played_steps > BATCH_SIZE * 34):\n",
    "        print(\"used in training: \", start_eps, \"-\", end_eps)\n",
    "        print(\"num of total steps in training \", played_steps)\n",
    "        start_of_train = timeit.default_timer()\n",
    "        rollouts = history.get_range(train_start, train_end)\n",
    "        print(\"training begin\")\n",
    "        learn(agent, rollouts)\n",
    "        end_of_train = timeit.default_timer()\n",
    "        exe_time_game_play = end_of_play - start_of_play\n",
    "        train_time_game_paly = end_of_train - start_of_train\n",
    "        exe_times.append([exe_time_game_play, train_time_game_paly])\n",
    "        train_start = end_eps\n",
    "        train_end = end_eps\n",
    "        played_steps = 0\n",
    "        start_of_play = timeit.default_timer()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
